# -*- coding: utf-8 -*-
"""tiktok final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13B9L-Jpay3xmVQKbPsGMlf-oZ0O1CiFk

1. importing and configuration
"""

!pip install transformers
import pandas as pd
import requests
import nltk
import os
import time
import numpy as np
import glob
import gzip
import json
import kagglehub
import re
import random
from tqdm import tqdm
from kagglehub import KaggleDatasetAdapter
from transformers import pipeline
from PIL import Image
from collections import Counter
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score
from scipy.sparse import hstack
from huggingface_hub import notebook_login

nltk.download('vader_lexicon')
notebook_login("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")

"""2. cleaning the kaggle dataset"""

file_path = "reviews.csv"
df_kaggle = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "denizbilginn/google-maps-restaurant-reviews",
  file_path,
)

print(df_kaggle.shape) # (1100, 6)

print(df_kaggle.dtypes) # check data type of each column
df_kaggle.isnull().values.any() # check for missing values: false
df_kaggle.duplicated().any() # check for exact duplicate rows: false
df_kaggle = df_kaggle.applymap(lambda s: s.lower() if isinstance(s, str) else s) # convert all string columns to lowercase

nltk.download("stopwords")
nltk.download("wordnet")

stop = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
  if not isinstance(text, str):
    return ""

  # remove leading/trailing whitespace
  text = text.strip()

  # remove punctuation, numbers, special characters
  text = re.sub(r"[^a-z\s]", "", text)

  # collapse multiple spaces
  text = re.sub(r"\s+", " ", text)

  # remove stopwords and lemmatize
  words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop]
  text = " ".join(words)

  return text

df_kaggle_cleaned = df_kaggle
df_kaggle_cleaned["clean_text"] = df_kaggle_cleaned["text"].apply(clean_text) # create a new column clean_text

df_kaggle_cleaned.rename(columns={
    "business_name": "location_name",
    "text": "review_text",
    "author_name": "review_id"
}, inplace=True)

df_kaggle_cleaned.head() # check first 5 rows of the dataset

df_kaggle_cleaned.to_csv("kaggle_cleaned.csv", index = False)

"""3. cleaning the google local reviews dataset"""

#nltk setup
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("punkt")

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

#functions set up
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)  # remove punctuation, numbers, special characters
    text = re.sub(r"\s+", " ", text).strip()  # collapse spaces
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]
    return " ".join(tokens)

def process_gzip_file_stream(file_path, max_samples=1000):
    """Process a .json.gz file line by line with reservoir sampling."""
    sampled_reviews = []
    try:
        with gzip.open(file_path, 'rt', encoding='utf-8') as f:
            for line in f:
                obj = json.loads(line)
                review = None
                for key in obj.keys():
                    if 'review' in key.lower() or 'text' in key.lower():
                        review = obj[key]
                        break
                if review is None:
                    continue

                # reservoir sampling
                if len(sampled_reviews) < max_samples:
                    sampled_reviews.append(review)
                else:
                    i = random.randint(0, len(sampled_reviews))
                    if i < max_samples:
                        sampled_reviews[i] = review

        # clean reviews
        sampled_reviews = [clean_text(r) for r in sampled_reviews]

        df = pd.DataFrame({
            "review_id": range(len(sampled_reviews)),
            "location_name": "Unknown Business",
            "review_text": sampled_reviews,
            "clean_text": sampled_reviews
        })
        return df
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return pd.DataFrame()

base_url = "https://mcauleylab.ucsd.edu/public_datasets/gdrive/googlelocal"
states = ["Alabama", "Alaska"] # add in more states to increase the dataset
os.makedirs("google_reviews", exist_ok=True)
max_samples_per_state = 2000 # sample size
master_csv = "master_csv.csv"

# remove existing master CSV
if os.path.exists(master_csv):
    os.remove(master_csv)

#download and process files
for state in states:
    file_name = f"review-{state}_10.json.gz"
    file_url = f"{base_url}/{file_name}"
    local_path = os.path.join("google_reviews", file_name)

    if not os.path.exists(local_path):
        print(f"Downloading {file_name}...")
        r = requests.get(file_url)
        if r.status_code == 200:
            with open(local_path, "wb") as f:
                f.write(r.content)
        else:
            print(f"Failed to download {file_name}: {r.status_code}")
            continue

    print(f"Processing {state}...")
    df_state = process_gzip_file_stream(local_path, max_samples=max_samples_per_state)

    if not df_state.empty:
        # append to master CSV
        df_state.to_csv(master_csv, index=False, mode='a', header=not os.path.exists(master_csv))
        print(f"Added {len(df_state)} reviews from {state} to master CSV")
    else:
        print(f"No reviews sampled for {state}")

print("master CSV:", master_csv)

#check
df = pd.read_csv(master_csv)
print(df.head())
print(df.shape)
print("Missing values:", df.isnull().sum())

"""4. model"""

# 1. configuration
# ----------------

INPUT_PATH = "master_csv.csv"
API_KEY = "AIzaSyCt_oTJoiaxadSJ27QrISs7gErSaibR888"  # API key
CATEGORIES = ["Spam", "Advertisement", "Irrelevant", "Rant/Fake Complaint", "Genuine Review"]

TEST_MODE = True # optional testing on a sample size; set to FALSE for the full dataset
SAMPLE_SIZE = 100 # change this sample size

# 2. loading datasets
# -------------------

# inputs must be either csv or compressed json files
def load_reviews(path):
    if os.path.isfile(path) and path.endswith(".csv"):
        # single csv
        print(f"detected CSV file: {path}")
        df = pd.read_csv(path)

    elif os.path.isdir(path):
        # folder of compressed json files
        print(f"detected folder: {path}")
        files = glob.glob(f"{path}/*.json.gz")
        print(f"Found {len(files)} JSON review files.")

        all_reviews = []
        for f in files:
            with gzip.open(f, "rt", encoding = "utf-8") as fh:
                for line in fh:
                    all_reviews.append(json.loads(line))
        df = pd.DataFrame(all_reviews)

    else:
        raise ValueError(f"invalid path: {path}.")

    return df

df = load_reviews(INPUT_PATH)
print(f"Loaded {len(df)} reviews.")

# 3. sampling for testing (optional)
# ----------------------------------

if TEST_MODE:
    if len(df) > SAMPLE_SIZE:
        df = df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)
        print(f"TEST MODE ON: Using random sample of {SAMPLE_SIZE} reviews.")
    else:
        print(f"Dataset has only {len(df)} reviews, using all.")
else:
    print("FULL MODE: Processing the entire dataset.")

# standardise column names
if "review" in df.columns:
    df.rename(columns={"review": "clean_text"}, inplace=True)
if "text" in df.columns and "clean_text" not in df.columns:
    df.rename(columns={"text": "clean_text"}, inplace=True)

# ensure required columns
if "business_name" not in df.columns:
    df["business_name"] = "Unknown"
df["clean_text"] = df["clean_text"].fillna("").astype(str)

# 4. store categories by google API
# -------------------------------
def get_store_category(store_name, retries=3):
    if not isinstance(store_name, str) or store_name.strip() == "" or store_name == "Unknown":
        return []
    url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
    params = {"query": store_name, "key": API_KEY}
    for _ in range(retries):
        try:
            resp = requests.get(url, params=params).json()
            if "results" in resp and len(resp["results"]) > 0:
                return resp["results"][0].get("types", [])
        except Exception as e:
            print(f"Error fetching category for {store_name}: {e}")
        time.sleep(1)
    return []

print("Fetching store categories...")
df["store_category"] = df["business_name"].apply(get_store_category)

# 5. LLM classification in batches
# --------------------------------

# load model
classifier = pipeline(
    "text2text-generation",
    model="google/flan-t5-large",
    device_map="auto"
)

def clean_output(output, categories):
    """Normalize model output to one of the defined categories."""
    output = output.strip().replace('"', '').replace("'", "")
    for c in categories:
        if c.lower() in output.lower():
            return c
    return "Unknown"

def batch_classify(df, categories, batch_size=32):
    """Classify reviews in batches to speed up inference."""
    results = []
    for start in tqdm(range(0, len(df), batch_size)):
        end = min(start + batch_size, len(df))
        batch = df.iloc[start:end]

        # build prompts
        prompts = []
        for _, row in batch.iterrows():
            review = row["clean_text"]
            store = row.get("business_name", "Unknown")
            store_category = row.get("store_category", [])

            if not isinstance(review, str) or not review.strip():
                prompts.append("Classify as Unknown.")
                continue

            prompt = f"""
            You are moderating Google location reviews.

            Store: {store}
            Store categories: {store_category}

            Classify the following review into one of the categories: {categories}.

            Rules:
            If the review is about the store's products or services -> Genuine Review
            If the review is about something unrelated to the store -> Irrelevant
            If the review is promotional or contains links -> Advertisement
            If the review is gibberish -> Spam
            If the review is a complaint but the user mentions they have never visited the business -> Rant/Fake Complaint

            Examples:
            Review: "Buy now at our website" -> Advertisement
            Review: "Visit us at https://www.tiktok123.com" -> Advertisement
            Review: "I hate teletubbies, they are so ugly" -> Irrelevant
            Review: "I heard that the food is nice, but I have not been there before" -> Rant/Fake Complaint
            Review: "Great coffee, cozy atmosphere for getting some work done" -> Genuine Review
            Review: "The service was really bad, but the food was good" -> Genuine Review
            Review: "kjvsndkcn,sdmc e $#17*" -> Spam

            Review: "{review}"
            Answer with exactly one category: Spam, Advertisement, Irrelevant, Rant/Fake Complaint, Genuine Review.
            """
            prompts.append(prompt)

        # run the batch at once
        outputs = classifier(prompts, max_new_tokens=10, do_sample=False)

        # clean outputs
        for out in outputs:
            label = clean_output(out["generated_text"], categories)
            results.append(label)

    return results

print("running batched LLM classification...")
df["llm_category"] = batch_classify(df, CATEGORIES, batch_size=32)
print("sample outputs:")
print(df[["clean_text", "llm_category"]].head())

# 6. policy flagging & quality scoring
# -------------------------------
print("Running ML-based policy flagging...")

# basic policy flags
df["review_len"] = df["clean_text"].apply(lambda x: len(x.split()))
df["policy_flags"] = df["review_len"].apply(lambda x: ["too_short"] if x < 3 else [])

# sentiment
sia = SentimentIntensityAnalyzer()
df["sentiment"] = df["clean_text"].apply(lambda x: sia.polarity_scores(x)["compound"])

# TF-IDF + randomforest
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df["clean_text"])
meta_features = df[["review_len", "sentiment"]].values
X = hstack([X_tfidf, meta_features])
labels = df["policy_flags"].apply(lambda x: 1 if len(x) > 0 else 0)

X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
print(f"Policy Model: Precision={precision_score(y_test, y_pred):.3f}, Recall={recall_score(y_test, y_pred):.3f}, F1={f1_score(y_test, y_pred):.3f}")

df["policy_violation"] = rf_model.predict(X)

# quality scoring
df["quality_score"] = df["review_len"]*0.7 - df["policy_violation"]*1.0
df["rank"] = df["quality_score"].rank(ascending=False)

# 7. save output
# --------------
if INPUT_PATH.endswith(".csv"):
    OUTPUT_FILE = INPUT_PATH.replace(".csv", "_classified.csv")
else:
    OUTPUT_FILE = "classified_reviews.csv"

df.to_csv(OUTPUT_FILE, index=False)
print(f"saved {OUTPUT_FILE} with categories, sentiment, and ranking.")